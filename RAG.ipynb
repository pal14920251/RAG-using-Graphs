{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e772c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install -U torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7293a6",
   "metadata": {},
   "source": [
    "Deepseek Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HF_API_TOKEN = os.getenv(\"HF_API_TOKEN\")\n",
    "\n",
    "client = InferenceClient(model=\"deepseek-ai/DeepSeek-R1\", token=HF_API_TOKEN)\n",
    "\n",
    "response = client.text_generation(\"Explain what a transformer is in AI.\", max_new_tokens=100)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37faa3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(question: str, context: str):\n",
    "\n",
    "    if context == None or context == \"\":\n",
    "        prompt = f\"\"\"Give a detailed answer to the following question. Question: {question}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Using the information contained in the context, give a detailed answer to the question.\n",
    "            Context: {context}.\n",
    "            Question: {question}\"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        # { \"role\": \"model\", \"content\": \"Recurrent Attention (RAG)** is a novel neural network architecture specifically designed\" }\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = tokenizer.encode(\n",
    "        formatted_prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=250,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    response = response[len(formatted_prompt) :]  # remove input prompt from reponse\n",
    "    response = response.replace(\"<eos>\", \"\")  # remove eos token\n",
    "    return response\n",
    "\n",
    "\n",
    "question = \"What is a transformer?\"\n",
    "print(inference(question=question, context=\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7180ba",
   "metadata": {},
   "source": [
    "Document loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99dbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dcec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"/home/eversberg/Downloads/1706.03762.pdf\"),\n",
    "    PyPDFLoader(\"/home/eversberg/Downloads/2005.11401.pdf\"),\n",
    "]\n",
    "pages = []\n",
    "for loader in loaders:\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=12)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bb079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275c4bf",
   "metadata": {},
   "source": [
    "Embeddings and vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ea525",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings\n",
    ")\n",
    "encoder = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2', model_kwargs = {'device': \"cpu\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = encoder.embed_query(\"RAG\")\n",
    "embeddings2 = encoder.embed_query(docs[0].page_content)\n",
    "print(np.dot(embeddings1, embeddings2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c228fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ceaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "faiss_db = FAISS.from_documents(docs, encoder, distance_strategy=DistanceStrategy.DOT_PRODUCT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4640da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is a transformer?\"\n",
    "retrieved_docs = faiss_db.similarity_search(question, k=5)\n",
    "context = \"\".join(doc.page_content + \"\\n\" for doc in retrieved_docs)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(inference(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For this answer I used the following documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ee563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
